---
title: "Predicting the manner in which people did exercises using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants"
output: html_document


---

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).



Read more: http://groupware.les.inf.puc-rio.br/har#ixzz40JbypJrl
```{r, message= FALSE, warning=FALSE}
library(rpart)
library(caret)
library(randomForest)
training <- read.csv('pml-training.csv')
testing <- read.csv('pml-testing.csv')
```

Training dataset has the first 7 variables that are only descriptive, and do not affect the performance of the model:
X,user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window

These variables are removed from the list of predictive variables.
```{r}
columns <- colnames(training)[-c(1:7)]
training <- training[columns]
```

Next step is to calculate the variance across the variables to check if some of the variables have zero variability and also do not impact the model.
```{r, message= FALSE,warning=FALSE}
var <- apply(training, 2, var, na.rm = TRUE )
toRemove <- var[is.na(var)][-7] # exclude classe variable from uninfluential variables 
```
It is also vital to check for missing values. In this dataset there are some missing values that are represented as an empty string, to make it consistent they are replaced by NA. Examination of number of NA values across the variables reveal that there is a number of variables that have 19216 NA values out of 19622 observations. The remaining 2% of observations that have values for these variable cannot be used to predict the model, thus, the variables that have 98% NAs are removed from the model. 

```{r}
training[training == ''] <- NA
naCount <- apply(training,2, is.na)
naCount1<- apply(naCount,2, sum)
toRem <- names(naCount1[!(naCount1 == 0)])
training <- training[,!colnames(training) %in% toRem]

#check for NA values after some of the variables were removed
sum(is.na(training))
```

The training data set is clean and ready to be used to create a model. For this classification problem two models were chosen: decision trees and random forest. Training data was split into 10 parts and each model was testing using 10-fold cross validation. Then the average accuracy was taken and compared between these models and the best performance model was chosen to apply to the whole training data set to predict test data.

```{r, cache = TRUE}
set.seed(1223)

# creating cross validation 10 folds
flds <- createFolds(training[,1], k = 10, list = TRUE, returnTrain = FALSE)
accuracyTree <- c()
accuracyRF <- c()

for (i in 1:10){
    testInd <- flds[[i]]
    test = training[testInd, ]
    train <- training[-testInd,]
    modelTree <- rpart(classe ~ ., data=train, method="class")
    predictionTree <- predict(modelTree, test, type = "class")
    acc <- confusionMatrix(predictionTree, training$classe[testInd])$overall['Accuracy']
    accuracyTree <- c(accuracyTree, acc)  
    
    modelRF <- randomForest(classe ~ ., data=train, method="class")
    predictionRF <- predict(modelRF, test, type = "class")
    acc1 <- confusionMatrix(predictionRF, training$classe[testInd])$overall['Accuracy']
    accuracyRF <- c(accuracyRF, acc1)  
}

meanAccTree <- mean(accuracyTree)
meanAccRf <- mean(accuracyRF)
```


```{r chunk_name, include=FALSE}
meanAccTree <-round(meanAccTree,3 )
meanAccRf <- round(meanAccRf,3)
```
10-fold cross validation gives average accuracy for decision tree: `r meanAccTree `.

10-fold cross validation gives average accuracy for random forest: `r meanAccRf `.

The accuracy score clearly indicates that random forest is predicting the class with more than 99% accuracy. This model is used on the whole training set to predict testing examples. Test set is preprocessed the same way as the training set.

```{r, cache= TRUE}
testing <- testing[,colnames(testing) ]
modelFinal <- randomForest(classe ~ ., data=training, method="class")
predictionTest <- predict(modelFinal,testing, type = 'class')

predictionTest
```